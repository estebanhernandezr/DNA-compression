{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gzip",
      "provenance": [],
      "collapsed_sections": [
        "wb8xCfbXUXPN",
        "qtJZOL89Vhg9"
      ],
      "authorship_tag": "ABX9TyM6k1UdG02m7alUd+9bpqUg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/estebanhernandezr/DNA-compression/blob/main/gzip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PDF2TXT**"
      ],
      "metadata": {
        "id": "wg5I2natVs4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0HFSYoqVxXj",
        "outputId": "327272fd-ef21-44de-9e08-7aa4c4ec3724"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (1.26.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2"
      ],
      "metadata": {
        "id": "z6Pvjmr0V5VJ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf2txt(filename, text='', ini=0, fin=0):\n",
        "    pdffileobj=open(filename,'rb')\n",
        "    pdfreader=PyPDF2.PdfFileReader(pdffileobj)\n",
        "    y=pdfreader.numPages\n",
        "\n",
        "    if ini < 0:\n",
        "        x = y+ini\n",
        "    else:\n",
        "        x = ini\n",
        "    if fin < 0:\n",
        "        y = y+fin\n",
        "    else:\n",
        "        y = y-fin\n",
        "\n",
        "    for i in range(x, y):\n",
        "        pageobj=pdfreader.getPage(i)\n",
        "        text+=pageobj.extractText()\n",
        "    #print(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "lH7X7ytCV7dt"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GZIP**"
      ],
      "metadata": {
        "id": "wb8xCfbXUXPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqaRZM-M9RTg",
        "outputId": "30ceaf8c-953f-46d3-af7e-fc0068fb02c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.7/dist-packages (2.3.6)\n"
          ]
        }
      ],
      "source": [
        "pip install bitarray"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bitarray import *\n",
        "from typing import BinaryIO, Dict, Sequence, Tuple"
      ],
      "metadata": {
        "id": "6GA7cy8l9Zp_"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTIONS\n",
        "\n",
        "def data_from_file(filename: str) -> BinaryIO:\n",
        "    with open(filename, 'rb') as input_file:\n",
        "        filedata = input_file.read()\n",
        "    return filedata\n",
        "\n",
        "def file_from_bin(filename: str, buffer: bytearray) -> None:\n",
        "    with open(filename, 'wb') as outfile:\n",
        "        outfile.write(buffer)\n",
        "        return None\n",
        "\n",
        "def bin_from_file(filename: str) -> BinaryIO:\n",
        "    filedata: bytearray = bitarray(endian='big')\n",
        "    with open(filename, 'rb') as input_file:\n",
        "        filedata.fromfile(input_file)\n",
        "    return filedata\n",
        "\n",
        "def inipad(symb: int, padsize: int, cad: str) -> BinaryIO:\n",
        "    pad: bytearray = bytearray(chr(symb), 'utf-8')\n",
        "    for i in range(padsize-1):\n",
        "        pad.append(pad[0])\n",
        "    return pad + cad"
      ],
      "metadata": {
        "id": "rjBg9EsDKKdh"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPRESSER CLASS\n",
        "\n",
        "class Compresser:\n",
        "    _n: int\n",
        "    _Ls: int\n",
        "    _symb: int\n",
        "    _compressed_string: bytearray\n",
        "\n",
        "    _huffman_dictionary: Dict\n",
        "    _compressed_huffman: bytearray\n",
        "\n",
        "    def __init__(self, n: int, Ls: int):\n",
        "        if (n < 0 or Ls < 0):\n",
        "\t\t\t      raise ValueError(\"Negative buffer sizes\")\n",
        "\n",
        "        self._n = n\n",
        "        self._Ls = Ls\n",
        "        self._symb = 32\n",
        "        self._compressed_string = bitarray(endian='big')\n",
        "\n",
        "        self._huffman_dictionary = {}\n",
        "        self._compressed_huffman = bitarray(endian='big')\n",
        "\n",
        "    def codify_word(self, pos: int, size: int, char: chr=None) -> str:\n",
        "        n: int = self._n\n",
        "        Ls: int = self._Ls\n",
        "        codeword = \"{0:0{width}b}\".format(pos, width=len(\"{0:b}\".format(n)))\n",
        "        codeword += \"{0:0{width}b}\".format(size, width=len(\"{0:b}\".format(Ls)))\n",
        "        return codeword\n",
        "\n",
        "    def rep_extension(self, search: bytearray, lookahead: bytearray) -> Sequence[int]:\n",
        "        n: int = self._n\n",
        "        Ls: int = self._Ls\n",
        "        pos: int = -1\n",
        "        size: int = 0\n",
        "        char: chr = ''\n",
        "        for prefixsize in range(1, min(n-Ls, len(lookahead))):\n",
        "            prefix: str = lookahead[:prefixsize]\n",
        "            p: int = search.rfind(prefix, 0, (n-Ls)+prefixsize-1)\n",
        "            if p >= 0:\n",
        "                pos = p\n",
        "                size = prefixsize\n",
        "                char = lookahead[size]\n",
        "            else:\n",
        "                break\n",
        "        return pos, size, char\n",
        "\n",
        "    def codify_cad(self, cad: str) -> None:\n",
        "        n: int = self._n\n",
        "        Ls: int = self._Ls\n",
        "        symb: int = self._symb\n",
        "        dictionary: Dict = self._huffman_dictionary\n",
        "\n",
        "        pcad: bytearray = inipad(symb, n-Ls, cad)\n",
        "        i: int = 0\n",
        "        while i < len(pcad)-(n-Ls):\n",
        "            triple: Sequence[int] = self.rep_extension(pcad[i:i+n], pcad[i+n-Ls:i+n])\n",
        "            pos: int = triple[0]\n",
        "            size: int = triple[1]\n",
        "            if (pos >= 0 and size > 1):\n",
        "                self._compressed_string.append(True)\n",
        "                if \"{0:0{width}b}\".format(pos, width=len(\"{0:b}\".format(n))) in dictionary:\n",
        "                    huffman_code: str = dictionary[\"{0:0{width}b}\".format(pos,\n",
        "                                                                          width=len(\"{0:b}\".format(n)))]\n",
        "                    bin_code: str = huffman_code + \"{0:0{width}b}\".format(size,\n",
        "                                                                          width=len(\"{0:b}\".format(Ls)))\n",
        "                elif len(dictionary) == 0:\n",
        "                    bin_code: str = self.codify_word(pos, size)\n",
        "                \n",
        "                for bit in bin_code:\n",
        "                    if bit == '1':\n",
        "                        self._compressed_string.append(True)\n",
        "                    else:\n",
        "                        self._compressed_string.append(False)\n",
        "                i += size\n",
        "            else:\n",
        "                self._compressed_string.append(False)\n",
        "                self._compressed_string.frombytes(bytes([pcad[i+n-Ls]]))\n",
        "                i += 1\n",
        "        return None\n",
        "\n",
        "    def compress(self, filename: str) -> None:\n",
        "        filedata: bytearray = data_from_file(filename) #CHECKED\n",
        "        \n",
        "        #distances = get_distances(filedata) #CHECKED - HUFFMAN PART\n",
        "        #counts = bl_count_from_distances(distances) #CHECKED - HUFFMAN PART\n",
        "        #huffman_tree = create_huffman_tree(counts) #CHECKED - HUFFMAN PART\n",
        "        #dictionary = dictionary_from_tree(huffman_tree, '', codeDictionary) #CHECKED - HUFFMAN PART\n",
        "        #self._huffman_dictionary = dictionary\n",
        "\n",
        "        #self.codify_huffmantree(huffman_tree, treeBuffer) #CHECKED - HUFFMAN PART\n",
        "        self.codify_cad(filedata) #CHECKED\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "oF20f25u9bj_"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DECOMPRESSER CLASS\n",
        "\n",
        "class Decompresser:\n",
        "    _n: int\n",
        "    _Ls: int\n",
        "    _symb: int\n",
        "    _decompressed_string: bitarray\n",
        "    \n",
        "    _huffman_dictionary: Dict\n",
        "    _decompressed_huffman: bitarray\n",
        "\n",
        "    def __init__(self, n: int, Ls: int):\n",
        "        if (n < 0 or Ls < 0):\n",
        "\t\t\t      raise ValueError(\"Negative buffer sizes\")\n",
        "\n",
        "        self._n = n\n",
        "        self._Ls = Ls\n",
        "        self._symb = 32\n",
        "        self._decompressed_string = bitarray(endian='big')\n",
        "\n",
        "        self._huffman_dictionary = {}\n",
        "        self._decompressed_huffman = bitarray(endian='big')\n",
        "\n",
        "    def decompress_cad(self, filedata: bytearray) -> None:\n",
        "        n: int = self._n\n",
        "        Ls: int = self._Ls\n",
        "        symb: int = self._symb\n",
        "        self._decompressed_string = inipad(symb, n-Ls, bytes())\n",
        "\n",
        "        k: int = 0\n",
        "        while len(filedata) >= 9:\n",
        "            flag_pair = filedata.pop(0)\n",
        "            if not flag_pair:\n",
        "                byte = filedata[0:8].tobytes()\n",
        "                self._decompressed_string += byte\n",
        "                del filedata[0:8]\n",
        "                k += 1\n",
        "            else:\n",
        "                position = ''\n",
        "                for i in range(0, len(\"{0:b}\".format(n))):\n",
        "                    bit = filedata.pop(0)\n",
        "                    if bit == True:\n",
        "                        position += '1'\n",
        "                    else:\n",
        "                        position += '0'\n",
        "\n",
        "                curbitsubstring = ''\n",
        "                stop = False\n",
        "                while len(self._huffman_dictionary) > 0 and stop == False:\n",
        "                    bit = filedata.pop(0)\n",
        "                    if bit == True:\n",
        "                        curbitsubstring+=\"1\"\n",
        "                    else:\n",
        "                        curbitsubstring+=\"0\"\n",
        "                    for key in self._huffman_dictionary:\n",
        "                        if self._huffman_dictionary[key] == str(curbitsubstring):\n",
        "                            position = key\n",
        "                            stop = True\n",
        "\n",
        "                length = ''\n",
        "                for i in range(0, len(\"{0:b}\".format(Ls))):\n",
        "                    bit = filedata.pop(0)\n",
        "                    if bit == True:\n",
        "                        length += '1'\n",
        "                    else:\n",
        "                        length += '0'\n",
        "\n",
        "                bestDistance = int(position, 2)\n",
        "                bestLength = int(length, 2)\n",
        "                for i in range(bestLength):\n",
        "                    self._decompressed_string.append(\n",
        "                        self._decompressed_string[k+bestDistance+i])\n",
        "                k += bestLength\n",
        "\n",
        "        self._decompressed_string = self._decompressed_string[n-Ls:]\n",
        "        return None\n",
        "\n",
        "    def decompress(self, filename: str):\n",
        "        filedata: bytearray = bin_from_file(filename)\n",
        "\n",
        "        #root = decodify_huffman_tree(filedata) - HUFFMAN PART\n",
        "        #dictionary_from_tree(root, '', codeDictionary) - HUFFMAN PART\n",
        "        \n",
        "        self.decompress_cad(filedata)"
      ],
      "metadata": {
        "id": "R0j6pGVjKFgw"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEST SUITE**"
      ],
      "metadata": {
        "id": "qtJZOL89Vhg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename1 = 'PDFs/Spanish [Spain].pdf'\n",
        "filename2 = 'PDFs/Italian [Italy].pdf'\n",
        "text1 = pdf2txt(filename1)\n",
        "text2 = pdf2txt(filename2, text1, ini=-2, fin=0)\n",
        "\n",
        "file=open('datos'+'.txt', 'w')\n",
        "file.write(text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh5mfXqkWF_W",
        "outputId": "e788e77a-f2ba-41d7-a75c-61f284dde493"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14788"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename: str = 'datos.txt'\n",
        "\n",
        "Compresor = Compresser(5000, 1000)\n",
        "Compresor.compress(filename)\n",
        "\n",
        "Compresor._compressed_string.fill()\n",
        "file_from_bin('compressed_file', Compresor._compressed_string)"
      ],
      "metadata": {
        "id": "O1jk4QIRAlg0"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename: str = 'compressed_file'\n",
        "\n",
        "Decompresor = Decompresser(5000, 1000)\n",
        "Decompresor.decompress(filename)\n",
        "\n",
        "file_from_bin('decompressed_file', Decompresor._decompressed_string)"
      ],
      "metadata": {
        "id": "65oi7I0ERCtx"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MATRIX CONSTRUCTION**"
      ],
      "metadata": {
        "id": "wQ9UZS6Qb649"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "BKkiSmwsh-Aa"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_delta(txt1, txt2, dicc):\n",
        "    L1 = dicc[txt1]\n",
        "    L2 = dicc[txt2]\n",
        "    delta = L1 - L2\n",
        "    return delta\n",
        "\n",
        "def calculate2(Aa, Ab, Ba, Bb, A, B, dicc):\n",
        "    deltaAb = calculate_delta(Ab, A, dicc)\n",
        "    deltaAa = calculate_delta(Aa, A, dicc)\n",
        "    deltaBa = calculate_delta(Ba, B, dicc)\n",
        "    deltaBb = calculate_delta(Bb, B, dicc)\n",
        "    \n",
        "    return ((deltaAb - deltaBb)/deltaBb)+((deltaBa-deltaAa)/deltaAa)\n"
      ],
      "metadata": {
        "id": "AR8T99cbeCMt"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relat_entropy(pdf_1, pdf_2, n, Ls):\n",
        "    pdffileobj_1=open(pdf_1,'rb')\n",
        "    pdfreader_1=PyPDF2.PdfFileReader(pdffileobj_1)\n",
        "    y_1=pdfreader_1.numPages\n",
        "    txt_1A = pdf2txt(pdf_1, fin=-2)\n",
        "    txt_1Aa = pdf2txt(pdf_1, txt_1A, ini=-2, fin=0)\n",
        "\n",
        "    pdffileobj_2=open(pdf_2,'rb')\n",
        "    pdfreader_2=PyPDF2.PdfFileReader(pdffileobj_2)\n",
        "    y_2=pdfreader_2.numPages\n",
        "    txt_2B = pdf2txt(pdf_2, fin=-2)\n",
        "    txt_2Bb = pdf2txt(pdf_2, txt_2B, ini=-2, fin=0)\n",
        "\n",
        "    txt_1Ab = pdf2txt(pdf_2, txt_1A, ini=-2, fin=0)\n",
        "    txt_2Ba = pdf2txt(pdf_1, txt_2B, ini=-2, fin=0)\n",
        "\n",
        "    file_A=open(\"TXTs/A.txt\", 'w')\n",
        "    file_A.write(txt_1A)\n",
        "\n",
        "    file_Aa=open(\"TXTs/Aa.txt\", 'w')\n",
        "    file_Aa.write(txt_1Aa)\n",
        "\n",
        "    file_B=open(\"TXTs/B.txt\", 'w')\n",
        "    file_B.write(txt_2B)\n",
        "\n",
        "    file_Bb=open(\"TXTs/Bb.txt\", 'w')\n",
        "    file_Bb.write(txt_2Bb)\n",
        "    \n",
        "    file_Ab=open(\"TXTs/Ab.txt\", 'w')\n",
        "    file_Ab.write(txt_1Ab)\n",
        "\n",
        "    file_Ab=open(\"TXTs/Ba.txt\", 'w')\n",
        "    file_Ab.write(txt_2Ba)\n",
        "\n",
        "    dictionario = {}\n",
        "\n",
        "    Compresor_A = Compresser(n, Ls)\n",
        "    Compresor_A.compress(\"TXTs/A.txt\")\n",
        "    dictionario[\"TXTs/A.txt\"]=len(Compresor_A._compressed_string)\n",
        "\n",
        "    Compresor_B = Compresser(n, Ls)\n",
        "    Compresor_B.compress(\"TXTs/B.txt\")\n",
        "    dictionario[\"TXTs/B.txt\"]=len(Compresor_B._compressed_string)\n",
        "\n",
        "    Compresor_Aa = Compresser(n, Ls)\n",
        "    Compresor_Aa.compress(\"TXTs/Aa.txt\")\n",
        "    dictionario[\"TXTs/Aa.txt\"]=len(Compresor_Aa._compressed_string)\n",
        "\n",
        "    Compresor_Bb = Compresser(n, Ls)\n",
        "    Compresor_Bb.compress(\"TXTs/Bb.txt\")\n",
        "    dictionario[\"TXTs/Bb.txt\"]=len(Compresor_Bb._compressed_string)\n",
        "\n",
        "    Compresor_Ab = Compresser(n, Ls)\n",
        "    Compresor_Ab.compress(\"TXTs/Ab.txt\")\n",
        "    dictionario[\"TXTs/Ab.txt\"]=len(Compresor_Ab._compressed_string)\n",
        "\n",
        "    Compresor_Ba = Compresser(n, Ls)\n",
        "    Compresor_Ba.compress(\"TXTs/Ba.txt\")\n",
        "    dictionario[\"TXTs/Ba.txt\"]=len(Compresor_Ba._compressed_string)\n",
        "\n",
        "    res = calculate2( 'TXTs/Aa.txt', 'TXTs/Ab.txt', 'TXTs/Ba.txt', 'TXTs/Bb.txt', 'TXTs/A.txt', 'TXTs/B.txt', dictionario)\n",
        "    print(pdf_1, \"(\",(res),\")\", pdf_2)\n",
        "    return res"
      ],
      "metadata": {
        "id": "ZnFDaZ6LW5iN"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = 2000\n",
        "ahead = 1000\n",
        "\n",
        "path = '/content/PDFs/'\n",
        "dir_path = os.path.dirname(os.path.realpath(path))\n",
        "\n",
        "PDFs = []\n",
        "for root, dirs, files in os.walk(dir_path):\n",
        "    for file_1 in files:\n",
        "        for file_2 in files:\n",
        "          if file_1.endswith('.pdf') and file_2.endswith('.pdf'):\n",
        "              relat_entropy(path+file_1, path+file_2, search, ahead)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIrrsOAUii7A",
        "outputId": "982038c4-862a-41ef-cf90-5238bc4f1108"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PDFs/English [UK].pdf ( 0.0 ) /content/PDFs/English [UK].pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PDFs/English [UK].pdf ( 0.09072326014585716 ) /content/PDFs/Italian [Italy].pdf\n",
            "/content/PDFs/English [UK].pdf ( 2.40969134249388 ) /content/PDFs/German [Germany].pdf\n",
            "/content/PDFs/English [UK].pdf ( 0.1449873550099888 ) /content/PDFs/Estonian [Estonia].pdf\n",
            "/content/PDFs/English [UK].pdf ( 0.08238484350757722 ) /content/PDFs/Spanish [Spain].pdf\n",
            "/content/PDFs/Italian [Italy].pdf ( 0.09072326014585716 ) /content/PDFs/English [UK].pdf\n",
            "/content/PDFs/Italian [Italy].pdf ( 0.0 ) /content/PDFs/Italian [Italy].pdf\n",
            "/content/PDFs/Italian [Italy].pdf ( 2.5638468701142383 ) /content/PDFs/German [Germany].pdf\n",
            "/content/PDFs/Italian [Italy].pdf ( 0.1315736286261255 ) /content/PDFs/Estonian [Estonia].pdf\n",
            "/content/PDFs/Italian [Italy].pdf ( 0.0803460847980605 ) /content/PDFs/Spanish [Spain].pdf\n",
            "/content/PDFs/German [Germany].pdf ( 2.40969134249388 ) /content/PDFs/English [UK].pdf\n",
            "/content/PDFs/German [Germany].pdf ( 2.5638468701142383 ) /content/PDFs/Italian [Italy].pdf\n",
            "/content/PDFs/German [Germany].pdf ( 0.0 ) /content/PDFs/German [Germany].pdf\n",
            "/content/PDFs/German [Germany].pdf ( 1.8997193013930547 ) /content/PDFs/Estonian [Estonia].pdf\n",
            "/content/PDFs/German [Germany].pdf ( 2.6832831371777712 ) /content/PDFs/Spanish [Spain].pdf\n",
            "/content/PDFs/Estonian [Estonia].pdf ( 0.1449873550099888 ) /content/PDFs/English [UK].pdf\n",
            "/content/PDFs/Estonian [Estonia].pdf ( 0.1315736286261255 ) /content/PDFs/Italian [Italy].pdf\n",
            "/content/PDFs/Estonian [Estonia].pdf ( 1.8997193013930547 ) /content/PDFs/German [Germany].pdf\n",
            "/content/PDFs/Estonian [Estonia].pdf ( 0.0 ) /content/PDFs/Estonian [Estonia].pdf\n",
            "/content/PDFs/Estonian [Estonia].pdf ( 0.12527448703018057 ) /content/PDFs/Spanish [Spain].pdf\n",
            "/content/PDFs/Spanish [Spain].pdf ( 0.08238484350757722 ) /content/PDFs/English [UK].pdf\n",
            "/content/PDFs/Spanish [Spain].pdf ( 0.0803460847980605 ) /content/PDFs/Italian [Italy].pdf\n",
            "/content/PDFs/Spanish [Spain].pdf ( 2.6832831371777712 ) /content/PDFs/German [Germany].pdf\n",
            "/content/PDFs/Spanish [Spain].pdf ( 0.12527448703018057 ) /content/PDFs/Estonian [Estonia].pdf\n",
            "/content/PDFs/Spanish [Spain].pdf ( 0.0 ) /content/PDFs/Spanish [Spain].pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/PDFs/'\n",
        "dir_path = os.path.dirname(os.path.realpath(path)) # <----- Inicializar con esta variable la ruta a la cual se le quieren extraer los archivos de determinado tipo\n",
        "\n",
        "PDFs = []\n",
        "for root, dirs, files in os.walk(dir_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.pdf'): # <----- # Especificamos el tipo de archivo que nos interesa extraer.\n",
        "            location = str(file)\n",
        "            location = location.replace(\"\\\\\", \"/\")\n",
        "            PDFs.append(location)\n",
        "\n",
        "# CREAMOS ARCHIVOS TXTs\n",
        "TXTs = []\n",
        "for (i, pdf1) in enumerate(PDFs):\n",
        "    for (j, pdf2) in enumerate(PDFs):\n",
        "        filename1 = path+pdf1\n",
        "        filename2 = path+pdf2\n",
        "        text1 = pdf2txt(filename1, fin=-2)\n",
        "        text2 = pdf2txt(filename2, text1, ini=-2, fin=0)\n",
        "\n",
        "        TXTs.append(\"TXTs/\"+str(i)+str(j)+'.txt')\n",
        "        file=open(\"TXTs/\"+str(i)+str(j)+'.txt', 'w')\n",
        "        file.write(text2)\n",
        "\n",
        "# CREAMOS ARCHIVOS TXTs\n",
        "for (i, pdf1) in enumerate(PDFs):\n",
        "    filename1 = path+pdf1\n",
        "    text1 = pdf2txt(filename1, fin=-2)\n",
        "\n",
        "    TXTs.append(\"TXTs/\"+str(i)+'.txt')\n",
        "    file=open(\"TXTs/\"+str(i)+'.txt', 'w')\n",
        "    file.write(text1)\n",
        "\n",
        "print(TXTs)\n",
        "# COMPRIMIMOS ARCHIVOS TXTs\n",
        "dictionario = {}\n",
        "for txt in TXTs:\n",
        "    Compresor = Compresser(2000, 1000)\n",
        "    Compresor.compress(txt)\n",
        "\n",
        "    print(txt)\n",
        "    print(len(Compresor._compressed_string))\n",
        "    dictionario[txt]=len(Compresor._compressed_string)\n",
        "\n",
        "print(dictionario)\n",
        "\n",
        "    \n",
        "\n",
        "res1 = calculate2( 'TXTs/00.txt', 'TXTs/01.txt', 'TXTs/10.txt', 'TXTs/11.txt', 'TXTs/0.txt', 'TXTs/1.txt', dictionario)\n",
        "res2 = calculate2( 'TXTs/00.txt', 'TXTs/02.txt', 'TXTs/20.txt', 'TXTs/22.txt', 'TXTs/0.txt', 'TXTs/2.txt', dictionario)\n",
        "res3 = calculate2( 'TXTs/22.txt', 'TXTs/21.txt', 'TXTs/12.txt', 'TXTs/11.txt', 'TXTs/2.txt', 'TXTs/1.txt', dictionario)\n",
        "\n",
        "print(\"ITALY-TURKISH\")\n",
        "print(res1)\n",
        "print(\"ITALY-SPANISH\")\n",
        "print(res2)\n",
        "print(\"TURKISH-SPAIN\")\n",
        "print(res3)"
      ],
      "metadata": {
        "id": "pEk-_dUrb6FN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178ee8eb-784a-44eb-c972-138711fbb95f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TXTs/00.txt', 'TXTs/01.txt', 'TXTs/02.txt', 'TXTs/03.txt', 'TXTs/10.txt', 'TXTs/11.txt', 'TXTs/12.txt', 'TXTs/13.txt', 'TXTs/20.txt', 'TXTs/21.txt', 'TXTs/22.txt', 'TXTs/23.txt', 'TXTs/30.txt', 'TXTs/31.txt', 'TXTs/32.txt', 'TXTs/33.txt', 'TXTs/0.txt', 'TXTs/1.txt', 'TXTs/2.txt', 'TXTs/3.txt']\n",
            "TXTs/00.txt\n",
            "62784\n",
            "TXTs/01.txt\n",
            "62729\n",
            "TXTs/02.txt\n",
            "75594\n",
            "TXTs/03.txt\n",
            "61891\n",
            "TXTs/10.txt\n",
            "77150\n",
            "TXTs/11.txt\n",
            "75717\n",
            "TXTs/12.txt\n",
            "89221\n",
            "TXTs/13.txt\n",
            "75787\n",
            "TXTs/20.txt\n",
            "62737\n",
            "TXTs/21.txt\n",
            "61952\n",
            "TXTs/22.txt\n",
            "73436\n",
            "TXTs/23.txt\n",
            "61227\n",
            "TXTs/30.txt\n",
            "72862\n",
            "TXTs/31.txt\n",
            "71822\n",
            "TXTs/32.txt\n",
            "84850\n",
            "TXTs/33.txt\n",
            "70727\n",
            "TXTs/0.txt\n",
            "47189\n",
            "TXTs/1.txt\n",
            "60889\n",
            "TXTs/2.txt\n",
            "46044\n",
            "TXTs/3.txt\n",
            "56478\n",
            "{'TXTs/00.txt': 62784, 'TXTs/01.txt': 62729, 'TXTs/02.txt': 75594, 'TXTs/03.txt': 61891, 'TXTs/10.txt': 77150, 'TXTs/11.txt': 75717, 'TXTs/12.txt': 89221, 'TXTs/13.txt': 75787, 'TXTs/20.txt': 62737, 'TXTs/21.txt': 61952, 'TXTs/22.txt': 73436, 'TXTs/23.txt': 61227, 'TXTs/30.txt': 72862, 'TXTs/31.txt': 71822, 'TXTs/32.txt': 84850, 'TXTs/33.txt': 70727, 'TXTs/0.txt': 47189, 'TXTs/1.txt': 60889, 'TXTs/2.txt': 46044, 'TXTs/3.txt': 56478}\n",
            "ITALY-TURKISH\n",
            "0.09072326014585716\n",
            "ITALY-SPANISH\n",
            "0.10738878225632467\n",
            "TURKISH-SPAIN\n",
            "0.10715176547779019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6KBlUh0ZbVZn"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEBUGGIN"
      ],
      "metadata": {
        "id": "lYZbysoe7j-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "flMuFB-B7mVB"
      },
      "execution_count": 95,
      "outputs": []
    }
  ]
}